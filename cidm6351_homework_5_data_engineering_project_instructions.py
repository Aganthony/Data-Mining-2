# -*- coding: utf-8 -*-
"""Amarachi Anthony  - CIDM6351 Homework 5 Data Engineering Project instructions

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CnFzhufXg8JMpPDSnt2xA3TAwocqByOE

# CIDM6351 Homework 5 Data Engineering Project Instructions

In this homework, you will perform a data engineering project on a dataset of your choosing.

Learning Objectives: Students will be able to...
1. load data from a variety of sources
2. perform data engineering tasks to clean and transform the data
3. analize the data to answer research questions
4. create visualizations that support the researchers conclusions
5. communicate in written form the main findings
6. Webscrap a website to gather data

Deliverable Tasks:   
1. Copy and save a copy of this Google Colab notebook to your notebook.
2. Change the Share's General Access of your notebook to "Anyone with a link" so that the professor can see your code.
3. When done, deliver the URL of your notebook to \\WTclass\cidm6351\lessons\Week 13-14\Turn in Homework 5\

Note there are two questions to this assignment. The first question is regarding a data project of your choosing. The second question is a task to webscrap data from a website identified by Dr. Humpherys.  <br>

## Project 1.1 Data Engineering on Data of Your Choosing

1. Find and import data to analize. Consider a topic that interest you. Consider searching Google with "dataset for machine learning" or search Kaggle for interesting datasets. https://www.kaggle.com/datasets

2. Explore your data. Get to know the data, structure, meaning of columns, and possible problems with the data.
3. Think of some research questions you may be able to answer using your data.
4. Make <b>at least 10 cleaning or transforming operations</b> on the data. This learning objective demonstrates your ability to data engineer. You choose the manipulations as you see fit. Cleaning and transforming can include but not limited to, filter, drop nulls, remove columns, rename columns, split columns into two, concantenate data, categorize, group by, agregate, create new columns, data type conversion, date manipulations, combining with other datasets, removing unwanted characters, etc. Importing and visualizations are not considered cleaning or transforming.
5. Label and number the 10 cleanning or transforming operation tasks with a comment so the professor can easily count your 10 operations and understand your code, e.g. "#1. create a new column 'Sales_2022' by summing the prior 12 months sales."
6. Note. There are lots of ways to store data to load in Colab. The downside is that when your Colab notebook disconnects from the runtime all your data is gone. The easiest way is to persist your data is to put your datafiles on github.com and link to the raw version of your datafiles. In your past assignments, you have been loading data from my github.com repo. Here is a 3 minute video on how to link to a raw file on github. https://www.screencast.com/t/vwmJmGRH
"""

# Enter your Project 1.1 code here

# The dataset contains Boston Police department crime incident report and focues on capturing the type of incident  and also where it occurred.
#  Data contains information about the crime such as date, location, crime group, crime code.

# Load data from the URL into a DataFrame
import pandas as pd
import matplotlib.pyplot as plt

url = "https://raw.githubusercontent.com/Aganthony/CIDM6351-Homework-5-Data-Engineering-Project-instructions/main/Boston%20Crime.csv"

# Specify the encoding when reading the CSV file
df = pd.read_csv(url, encoding='ISO-8859-1')

# Now you can perform operations on the DataFrame, such as positional indexing
# For example, to get the first 5 rows:
subset = df.iloc[:5]

print(subset)

# 1 Rename Column 'INCIDENT_NUMBER' to 'Incident_No'
df.rename(columns={'INCIDENT_NUMBER': 'Incident_No'},inplace=True)
print(df)

# 2 Rename Column 'OFFENSE_CODE' to 'Offence_code'
df.rename(columns={'OFFENSE_CODE': 'Offence_code'},inplace=True)
print(df)

# 3 Dropping the 'SHOOTING' column as it seems to have all NaN values
df.drop(columns=['SHOOTING'], inplace=True)
print(df)

# 4. Splitting the 'OCCURRED_ON_DATE' column into 'Date' and 'Time' columns
df[['Date', 'Time']] = df['OCCURRED_ON_DATE'].str.split(' ', expand=True)
df.drop(columns=['OCCURRED_ON_DATE'], inplace=True)
print(df)

 # 5. Converting 'Date' column to datetime format
df['Date'] = pd.to_datetime(df['Date'])
print(df)

# 6. Extracting year, month, and day from the 'Date' column
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['Day'] = df['Date'].dt.day
print(df)

# 7. Removing unwanted characters from the 'Location' column
df['Location'] = df['Location'].str.replace('(', '').str.replace(')', '')
print(df)

# 8. Renaming columns for better readability
df.rename(columns={'Lat': 'Latitude', 'Long': 'Longitude'}, inplace=True)
print(df)

# 9 Rename Column 'REPORTING_AREA ' to 'Zip_Code'
df.rename(columns={'REPORTING_AREA ': 'Zip_Code'},inplace=True)
print(df)

# 10: Grouping and aggregation for total offenses per day of the week
offenses_per_day = df.groupby('DAY_OF_WEEK').size().reset_index(name='Total_Offenses')
print(df)

"""## Project 1.2 Analize Your Data

1. Answer at least two research questions that can be answered using your cleaned and transformed data. Use a textbox to write your two questions and brief answer to the questions. Example: "What are the sales for 2022? Sales were $2 million, an increase of 10% over 2021. " Edit the textbox labeled "Project 1.3 Research Question and Answers" to add your research questions and major findings.
2. Create a simple visualization(s) that supports your findings.
"""

# Enter the code for your Project 1.2 visualizations here.

import matplotlib.pyplot as plt
# Visualization for Research Question 1: Average occurrence hour for different offense groups
plt.figure(figsize=(10, 6))
df.groupby('OFFENSE_CODE_GROUP')['HOUR'].mean().sort_values().plot(kind='barh', color='skyblue')
plt.xlabel('Average Occurrence Hour')
plt.ylabel('Offense Code Group')
plt.title('Average Occurrence Hour for Different Offense Groups')
plt.grid(axis='x')
plt.show()

# Visualization for Research Question 2: Total offenses per day of the week
plt.figure(figsize=(8, 6))
offenses_per_day.plot(kind='bar', x='DAY_OF_WEEK', y='Total_Offenses', color='lightgreen')
plt.xlabel('Day of the Week')
plt.ylabel('Total Number of Offenses')
plt.title('Total Offenses per Day of the Week')
plt.xticks(rotation=45)
plt.grid(axis='y')
plt.show()

"""Project 1.3 Research Questions and Answers
Write your research questions and answers here. Two paragraphs is sufficient.

Research Question 1:

What is the average occurrence hour for different offense groups?

Findings: The average occurrence hour varies across offense groups. For instance, 'Larceny' tends to occur around noon, while 'Simple Assault' and 'Investigate Person' offenses happen more frequently in the afternoon.

Research Question 2:

Which day of the week records the highest number of offenses?
Findings:

 The analysis reveals that Saturday has the highest number of offenses, followed closely by Sunday, whereas Tuesday has the lowest number of recorded offenses.

## Project 2: Web Scraping

President Wendler has proposed facutly at WT use Open Educational Resources (OER) or instructor created content to lower the cost of text-books and other educational material. To help decision makers, we want to analize what the University of Houston has done regarding OER. The University of Houston offers grant awards to faculty who convert their courses to OER and OER-like content while maintaining the quality of the educational materials. Your task is to perform webscrapping on their website to analyze the data.

Tasks:
1. Web scrape the four tables of data at https://guides.lib.uh.edu/OER/ATIP/awardees
2. Store the data from all four tables into one pandas dataframe with columns "Year", "Name", "College", and "Course". Sort the data by year.

Data Dictionary:<br>
*Year* is the year the award was granted, e.g. "2022"<br>
*Name* is the text in the Name column. You are not required to split the faculty names. What ever text is in the name column, use that value, e.g., "Chiara Acquati, Aabha Brown, and Ginger Lucas"<br>
*College*. Name of the college from the College column, e.g., "Liberal Arts & Social Sciences"<br>
*Course*. Prefix, Number and Name of the course, e.g., "PSYC 2305: Introduction to Methods in Psychology"<br>

3. Think of a research question you can analize with this data. In a Colab textbox "Project 2.2", write your research question and answer. One paragraph will be sufficient.

4. Create a simple visualization to support your major finding from the research question.

5. The hardest part of this project is that there are four tables without unique identifiers. Pandas and beautiful soup may return a list of tables that you have to manage. Consider learning about the "Pandas read_html()" method. Search that term and possibly "pandas read multiple tables into dataframe".

### Resources
1. You learned about request and beautiful soup in datacamp and in <a href="https://docs.google.com/document/d/1tnX9QWzl9_nbnfhzbbS7PP7Gnmp4TMnmAEmkOFU1gh4/edit?usp=sharing"> CIDM6303 Lesson 11</a>
2. Dr. Humpherys shared a <a href="https://www.screencast.com/t/tOA2g2tV" >video of a code walkthrough</a> collecting consumer data from auto websites.
3. Consider searching Google and youtube for "python web scrapping"
4. Datacamp has an article on <a href="https://www.datacamp.com/tutorial/scraping-reddit-python-scrapy?utm_source=google&utm_medium=paid_search&utm_campaignid=19589720830&utm_adgroupid=157156377311&utm_device=c&utm_keyword=&utm_matchtype=&utm_network=g&utm_adpostion=&utm_creative=676354849673&utm_targetid=dsa-2218886984100&utm_loc_interest_ms=&utm_loc_physical_ms=9028461&utm_content=&utm_campaign=230119_1-sea~dsa~tofu_2-b2c_3-us_4-prc_5-na_6-na_7-le_8-pdsh-go_9-na_10-na_11-na-oct23&gclid=Cj0KCQjwy4KqBhD0ARIsAEbCt6gnUPZDC4xg8cq5CLElNibxJM8KJE6PlWm5pm7Qpyzu7VeOUCm1pSQaAr8ZEALw_wcB" target="blank">web scraping here </a>
4. Datacamp has an extra course just on web scraping. https://app.datacamp.com/learn/courses/web-scraping-with-python
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd

# Send a GET request to the URL
url = "https://guides.lib.uh.edu/OER/ATIP/awardees"
response = requests.get(url)

# Parse the HTML content using BeautifulSoup
soup = BeautifulSoup(response.content, "html.parser")

# Find all tables on the page
tables = soup.find_all("table")

# Extract data from tables
data = []
for table in tables:
    rows = table.find_all("tr")
    for row in rows[1:]:  # Skip the header row
        cols = row.find_all("td")
        data.append([col.text.strip() for col in cols])

# Creating a DataFrame from the extracted data
df = pd.DataFrame(data, columns=["Name", "College", "Course"])

# Determining the default starting year and total years from 2018 to 2022
start_year = 2018
end_year = 2022
num_years = end_year - start_year + 1

# Adding 'Year' column based on the reference years
year_index = 0
rows_per_year = len(df) // num_years
for i in range(len(df)):
    year_index = (i // rows_per_year) if i < rows_per_year * num_years else num_years - 1
    df.loc[i, 'Year'] = start_year + year_index

# Converting 'Year' column to integers to remove decimal part
df['Year'] = df['Year'].astype(int)

# Sorting the DataFrame by 'Year' asceding
df = df.sort_values('Year')

# Displaying result from the DataFrame
print(df)

"""###Project 2.2 Research Question and Answer.
Enter your research question and answer in this textbox.<br>

Reasearch questin 1: What is the movement in the variety of awarded courses across different colleges from 2018 to 2022?

Answer: During the years 2018 to 2022, colleges have expanded their course offerings to include more specialized and diverse courses. A visualization of the count or distribution of courses across different colleges in each year would provide a clearer understanding of this trend.


Research Question 2:
"How has the distribution of awarded projects in the field of Open Educational Resources (OER) varied across academic years from 2018 to 2022 at [University/Institution Name]?"

Answer:
The analysis of awarded projects in the realm of Open Educational Resources (OER) across the academic years from 2018 to 2022 at [University/Institution Name] reflects notable shifts in the distribution of projects. The data reveals an uneven pattern, with a surge in awarded projects observed in the academic year 2019-2020, followed by a relatively steady trend in the subsequent years. Interestingly, there appears to be a decline in the number of awarded projects in the academic year 2021-2022, marking a potential deviation from the upward trend observed in the previous years.

"""

import matplotlib.pyplot as plt

# Calculating the project counts per year
project_counts = df['Year'].value_counts().sort_index()

# Creating a bar chart
plt.figure(figsize=(8, 6))
plt.bar(project_counts.index.astype(str), project_counts.values, color='skyblue')
plt.xlabel('Academic Year')
plt.ylabel('Number of Awarded Projects')
plt.title('Distribution of Awarded Projects across Academic Years')
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.tight_layout()

# Show the plot
plt.show()

"""## DONE!

Upload the url of your notebook to WTclass.

## Grading Rubric:  

Project 1 is 50%. Project 2 is 50%. <br>
Project 1.1 is 30%, which includes 2% for each cleaning/transforming operation.<br>
Project 1.2 is 10% for working code. <br>
Project 1.3 is 10%, which includes 5% for each of two research questions, answers, and respective visualization(s). <br>
Project 2.1 is 40% for working code as instructed. <br>
Project 2.2 is 10% research question, answer, and respective visualization.<br>
Partial credit may be awarded.  



"""